## 倹 Subtopic 3.2: Advanced Feature Engineering Techniques

**Goal:** Create powerful and predictive features beyond simple one-hot encoding or basic aggregations, including interaction terms, sophisticated encodings for categoricals, and robust feature selection methods.

**Resources:**

* **Scikit-learn Preprocessing:** [User Guide](https://scikit-learn.org/stable/modules/preprocessing.html) (PolynomialFeatures, TargetEncoder)
* **Category Encoders Library:** [Documentation](https://contrib.scikit-learn.org/category_encoders/) (TargetEncoder, LeaveOneOutEncoder, CatBoostEncoder, etc.)
* **Feature Selection:** [Scikit-learn Guide](https://scikit-learn.org/stable/modules/feature_selection.html), [BorutaPy Library](https://github.com/scikit-learn-contrib/boruta_py), [Permutation Importance](https://scikit-learn.org/stable/modules/permutation_importance.html)
* **Interaction Features Blog:** [Examples and explanations](https://christophm.github.io/interpretable-ml-book/interaction.html)

---

### 隼 **Exercise 1: Polynomial and Interaction Features**

**Goal:** Generate polynomial features and explicit interaction terms to capture non-linear relationships between numerical features.

**Instructions:**

1.  Load a dataset with several numerical features (e.g., Boston Housing, Diabetes dataset).
2.  Select two numerical features, `feat_A` and `feat_B`.
3.  Use `sklearn.preprocessing.PolynomialFeatures` with `degree=2` and `interaction_only=False` on these two features. Examine the output columns – identify the original features, squared terms (`A^2`, `B^2`), and the interaction term (`A*B`).
4.  Now use `PolynomialFeatures` with `degree=2` and `interaction_only=True`. What columns are generated now?
5.  Train a simple linear model (e.g., Linear Regression or Ridge) using:
    * Only the original features `feat_A`, `feat_B`.
    * The original features plus the features generated by `PolynomialFeatures(degree=2, interaction_only=False)`.
    * The original features plus only the interaction term (`A*B`) created manually.
6.  Evaluate the models using cross-validation (e.g., RMSE or R2 score). Did adding polynomial/interaction terms improve performance?
7.  Discuss the risk of increasing the degree in `PolynomialFeatures`. What problem can arise?
8.  **Challenge:** Implement manual creation of interaction terms between *all pairs* of numerical features in your dataset. How does the number of features grow?

---

### 隼 **Exercise 2: Advanced Categorical Encoding (Target/LeaveOneOut)**

**Goal:** Implement and compare Target Encoding and Leave-One-Out Encoding, considering strategies to prevent target leakage.

**Instructions:**

1.  Load a dataset with categorical features and a target variable (e.g., Titanic dataset, Ames Housing). Select a high-cardinality categorical feature.
2.  Implement **Target Encoding** using the `category_encoders` library (`TargetEncoder`). Ensure you use it correctly within a cross-validation pipeline (as discussed in Subtopic 3.1) to prevent leakage. Train a model (e.g., LightGBM) using this encoding.
3.  Implement **Leave-One-Out (LOO) Encoding** using `category_encoders` (`LeaveOneOutEncoder`). This calculates the target mean for a category excluding the current row itself. Train the same model using LOO encoding (again, correctly within CV).
4.  Compare the cross-validated performance of the model using Target Encoding vs. LOO Encoding vs. standard One-Hot Encoding for the chosen categorical feature.
5.  Discuss the pros and cons of Target Encoding vs. LOO Encoding. Which might be more prone to overfitting? How does smoothing affect Target Encoding?
6.  **Challenge:** Implement CatBoost Encoding (`CatBoostEncoder` from `category_encoders`). How does it differ from standard Target Encoding in its approach to preventing leakage (Hint: involves ordering/time)? Compare its performance.

---

### 隼 **Exercise 3: Permutation Importance**

**Goal:** Use permutation importance to evaluate feature importance after a model has been trained, providing insights that are model-agnostic.

**Instructions:**

1.  Train a moderately complex model (e.g., Random Forest, Gradient Boosting) on a dataset with a reasonable number of features.
2.  Use scikit-learn's `permutation_importance` function:
    * Pass the trained model, the validation/test set (X_val, y_val), the scoring metric (e.g., 'accuracy', 'r2', 'neg_mean_squared_error'), and the number of repeats (`n_repeats`) for stability.
3.  The function returns an object containing importance means, standard deviations, and raw values. Extract the importance mean for each feature.
4.  Rank the features based on their permutation importance (higher score drop means more important). Print the top N features.
5.  Compare the feature importances obtained from permutation importance to the built-in feature importances provided by tree-based models (e.g., `model.feature_importances_`). Are the rankings similar?
6.  Discuss the advantages of permutation importance: Why is it considered more reliable than impurity-based importance (like default tree importances) especially when features are correlated or have different scales?
7.  **Challenge:** Calculate permutation importance using the *training* data instead of the validation/test data. Are the results significantly different? Why might evaluating on unseen data be preferred?

---

### 隼 **Exercise 4: Feature Selection with Boruta**

**Goal:** Apply the Boruta algorithm for robust all-relevant feature selection.

**Instructions:**

1.  Install the `BorutaPy` library (`pip install BorutaPy`).
2.  Load a dataset, potentially with many features (some irrelevant or redundant).
3.  Choose an estimator compatible with Boruta (typically a Random Forest classifier or regressor).
4.  Instantiate the `BorutaPy` selector, passing the estimator and optional parameters like `max_iter`.
5.  Fit the `BorutaPy` object to your training data (`X_train`, `y_train`). This involves creating shadow features and iteratively comparing real feature importance against shadow feature importance.
6.  Check the `support_` attribute of the fitted Boruta object – this boolean mask indicates which features were confirmed as important.
7.  Check the `support_weak_` attribute – this indicates features deemed tentatively important.
8.  Check the `ranking_` attribute to see the selection rank.
9.  Select only the confirmed important features (`X_train_selected = X_train.loc[:, selector.support_]`).
10. Train your final model using only the selected features and evaluate its performance compared to using all features. Did Boruta successfully remove uninformative features without hurting performance significantly?
11. **Challenge:** How does Boruta differ from simpler feature selection methods like SelectKBest or Recursive Feature Elimination (RFE)? What does "all-relevant" mean in the context of Boruta?

---

### 隼 **Exercise 5: Manual Feature Interaction Creation**

**Goal:** Brainstorm and implement domain-specific or hypothesis-driven feature interactions manually.

**Instructions:**

1.  Choose a dataset where you have some domain knowledge or can form hypotheses (e.g., Titanic, House Prices).
2.  Brainstorm potential interaction features based on intuition:
    * **Titanic:** Could 'Age' * 'Pclass' be relevant (older people in lower classes)? Could 'FamilySize' (derived from 'SibSp' + 'Parch') interact with 'Fare'?
    * **House Prices:** Could 'TotalSquareFeet' (sum of basement, 1st, 2nd floor) be useful? Could 'OverallQuality' * 'YearBuilt' capture something? Could 'BathroomsPerBedroom' be informative?
3.  Implement the creation of these new interaction features using pandas operations (e.g., `df['Age_x_Pclass'] = df['Age'] * df['Pclass']`). Handle potential missing values appropriately during creation.
4.  Add these manually created features to your dataset.
5.  Train a model both with and without these manual interaction features.
6.  Evaluate if adding these specific, hypothesis-driven features improved model performance more effectively than blindly adding all polynomial interactions (as in Exercise 1).
7.  **Challenge:** Use feature importance techniques (permutation importance or model-based) to assess whether your manually created interaction features were indeed deemed important by the model.

---