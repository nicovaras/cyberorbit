## 倹 Subtopic 4.5: VI Implementation Techniques

**Goal:** Understand common practical techniques and assumptions in Variational Inference, including the Mean-Field approximation, Automatic Differentiation VI (ADVI), and Stochastic VI (SVI) for large datasets.

**Resources:**

* **Mean-Field Approximation:** Explanations in VI review papers (e.g., Blei et al., 2016).
* **ADVI Paper:** [Kucukelbir et al., 2017 - Automatic Differentiation Variational Inference](https://arxiv.org/abs/1603.00788)
* **Stochastic Variational Inference (SVI) Paper:** [Hoffman et al., 2013](https://arxiv.org/abs/1206.7051)
* **PPL Documentation:** (PyMC, NumPyro, Stan guides on VI, ADVI, mini-batch/SVI options)

---

### 隼 **Exercise 1: The Mean-Field Approximation**

**Goal:** Understand the simplifying assumption made by the standard Mean-Field Variational Bayes approach.

**Instructions:**

1.  Let `theta = (theta_1, theta_2, ..., theta_k)` be the set of model parameters.
2.  The Mean-Field assumption approximates the true posterior `p(theta|D)` with a variational distribution `q(theta)` that **factorizes** across the parameters (or groups of parameters):
    `q(theta) = q_1(theta_1) * q_2(theta_2) * ... * q_k(theta_k)`
3.  Explain what this assumption implies about the relationship between the parameters `theta_i` in the *approximate* posterior distribution `q`. (Hint: Independence).
4.  Consider the Bivariate Normal posterior from Subtopic 4.3 Exercise 1, where `theta_1` and `theta_2` were correlated (`rho=0.8`). If you were to approximate this using a mean-field approach where `q(theta_1, theta_2) = q_1(theta_1) * q_2(theta_2)`, what shape would the approximate posterior have? (Assume `q_1` and `q_2` are Normal). How would this compare to the true elliptical posterior? Draw a conceptual diagram.
5.  Discuss the main limitation introduced by the mean-field assumption. In what situations might it produce a poor approximation of the true posterior? (Hint: Strong posterior correlations).
6.  **Challenge:** Despite its limitations, why is the mean-field assumption so commonly used in practice for VI? (Hint: Simplifies the optimization problem).

---

### 隼 **Exercise 2: Automatic Differentiation VI (ADVI) Concept**

**Goal:** Understand the key ideas behind ADVI, which makes VI applicable to a wide range of models without requiring manual derivation of updates.

**Instructions:**

1.  Recall the ELBO objective function from Subtopic 4.4. ADVI typically uses a fixed variational family (often factorized Normals - Mean-Field) and transforms the parameters to an unconstrained space.
2.  Explain the role of **Automatic Differentiation (AD)** libraries (like those underlying PyTorch, TensorFlow, JAX) in ADVI. How do they allow the computation of gradients of the ELBO with respect to the variational parameters `phi`?
3.  Explain the **reparameterization trick**. Why is it difficult to compute the gradient of `E_q [ log p(D, theta) - log q(theta; phi) ]` directly if `q` is stochastic? How does reparameterizing the sampling process (e.g., for a Normal `q`, sampling `epsilon ~ N(0,1)` and setting `theta = mu + sigma * epsilon`) allow gradients to flow back to the parameters `phi = (mu, sigma)`?
4.  Outline the ADVI optimization loop:
    * Sample from the base distribution (e.g., standard Normal) using the reparameterization trick to get `theta` samples based on current `phi`.
    * Calculate the ELBO (or an estimate of it using Monte Carlo samples).
    * Use AD to compute the gradient of the ELBO with respect to `phi`.
    * Update `phi` using a gradient-based optimizer (e.g., Adam).
    * Repeat.
5.  Why does this combination of AD and the reparameterization trick make VI "automatic" and applicable to complex models defined in PPLs?
6.  **Challenge:** For which common distributions besides the Normal distribution can the reparameterization trick be easily applied? Are there distributions where it's difficult?

---

### 隼 **Exercise 3: Stochastic Variational Inference (SVI) for Large Datasets**

**Goal:** Understand how SVI enables VI to scale to large datasets by using mini-batches.

**Instructions:**

1.  Consider the ELBO calculation again. The `log p(D, theta)` term involves `log p(D|theta) + log p(theta)`. The likelihood term `log p(D|theta)` typically involves a sum over all `N` data points: `sum_{i=1 to N} log p(d_i | theta)`.
2.  Explain why calculating the ELBO and its gradient becomes computationally expensive when `N` (dataset size) is very large.
3.  Explain the core idea of SVI: Instead of using the full dataset to estimate the ELBO gradient at each step, use a **mini-batch** of data (`M << N` points).
4.  How is the likelihood term `sum_{i=1 to N} log p(d_i | theta)` estimated using a mini-batch? How does this estimate need to be scaled? (Hint: `(N/M) * sum_{j=1 to M} log p(d_j | theta)`).
5.  Describe the SVI optimization loop: How does it combine mini-batch processing with gradient updates to the variational parameters `phi`? What kind of optimizer is typically used (stochastic gradient ascent)?
6.  Why is SVI crucial for applying Bayesian methods (via VI) to large datasets where full MCMC or standard VI would be too slow?
7.  **Challenge:** Implement SVI using a PPL that supports it (e.g., PyMC with `pm.Minibatch`, Pyro/NumPyro). Train a simple model (e.g., Bayesian Linear Regression) on a moderately large dataset using SVI and compare its runtime and results to standard ADVI (if feasible) or MCMC.

---

### 隼 **Exercise 4: Choosing VI vs. MCMC in Practice**

**Goal:** Develop criteria for deciding when to use Variational Inference versus MCMC for a given Bayesian modeling task.

**Instructions:**

1.  Based on the previous exercises and subtopics, create a comparison table summarizing the pros and cons of VI (specifically ADVI/SVI) and MCMC (specifically NUTS/HMC):
    * **Speed:** Which is generally faster?
    * **Scalability (Dataset Size):** Which handles large datasets better?
    * **Scalability (Parameter Dimension):** How do they compare for high-dimensional models?
    * **Approximation Quality:** Which generally provides a more accurate approximation of the true posterior? Which might struggle with complex posterior geometries (multimodality, correlations)?
    * **Ease of Implementation:** (Assuming use of modern PPLs) Are they comparable?
    * **Convergence Diagnostics:** Which has more established and reliable convergence diagnostics?
2.  Describe scenarios where VI might be the preferred choice (e.g., large datasets, need for speed, embedding Bayesian methods in larger systems).
3.  Describe scenarios where MCMC might be necessary or preferred (e.g., requiring high accuracy for the posterior, complex posteriors where VI assumptions might fail, needing robust diagnostics).
4.  **Challenge:** Can VI and MCMC be used together? (Hint: Research using VI to initialize MCMC).

---